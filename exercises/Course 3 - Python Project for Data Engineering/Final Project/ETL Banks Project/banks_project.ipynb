{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "666a1c6d",
   "metadata": {},
   "source": [
    "# üè¶ ETL Project: Largest Banks Data Pipeline\n",
    "\n",
    "This notebook performs a full **Extract‚ÄìTransform‚ÄìLoad (ETL)** process on global banking data from Wikipedia.  \n",
    "It scrapes the table of the world‚Äôs largest banks by market capitalization, converts currencies using live exchange rates, and loads the final dataset into both a **CSV file** and a **SQLite database**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c144569",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Import Required Libraries\n",
    "\n",
    "We‚Äôll use popular Python libraries for web scraping, data manipulation, and database operations:\n",
    "- `requests` and `BeautifulSoup` for web scraping  \n",
    "- `pandas` and `numpy` for data transformation  \n",
    "- `sqlite3` for database loading  \n",
    "- `datetime` for logging process timestamps  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd4c8d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from io import StringIO\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29bea35",
   "metadata": {},
   "source": [
    "## üåê Configuration and Global Variables\n",
    "\n",
    "Below are the file paths, URLs, headers, and table attribute names used throughout the ETL pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41818911",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/List_of_largest_banks'\n",
    "header = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "attributes = [\"Name\", \"MC_USD_Billion\"]\n",
    "table_attributes = [\"Name\", \"MC_USD_Billion\",\n",
    "                    \"MC_GBP_Billion\", \"MC_EUR_Billion\", \"MC_INR_Billion\"]\n",
    "\n",
    "output_csv_path = \"Largest_banks_data.csv\"\n",
    "database_name = \"Banks.db\"\n",
    "table_name = \"Largest_banks\"\n",
    "log_file = \"code_log.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda3ab14",
   "metadata": {},
   "source": [
    "## üß± Helper Functions\n",
    "\n",
    "We‚Äôll define the following functions:\n",
    "- `log_progress()` ‚Äî Records process updates with timestamps.\n",
    "- `extract()` ‚Äî Fetches and parses the table from Wikipedia.\n",
    "- `transform()` ‚Äî Cleans the dataset and converts values to multiple currencies.\n",
    "- `load_to_csv()` ‚Äî Saves the transformed data to a CSV file.\n",
    "- `load_to_db()` ‚Äî Loads the data into a SQLite database.\n",
    "- `run_queries()` ‚Äî Executes SQL queries for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7879ac94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_progress(message):\n",
    "    timestamp_format = '%Y-%m-%d %H:%M:%S'\n",
    "    now = datetime.now()\n",
    "    timestamp = now.strftime(timestamp_format)\n",
    "    with open(log_file, \"a\") as file:\n",
    "        file.write(f\"{timestamp}: {message}\\n\")\n",
    "\n",
    "\n",
    "def extract(url):\n",
    "    r = requests.get(url, headers=header)\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    tables = str(soup.find_all(\"table\", class_=\"wikitable\"))\n",
    "    dfs = pd.read_html(StringIO(tables))\n",
    "    df = dfs[0]\n",
    "    return df\n",
    "\n",
    "\n",
    "def transform(df, csv_path):\n",
    "    rates = pd.read_csv(csv_path, index_col=\"Currency\")\n",
    "    df = df.rename(columns={\n",
    "        \"Bank name\": \"Name\", \"Total assets (2025) (US$ billion)\": \"MC_USD_Billion\"})\n",
    "    for currency, rate in rates[\"Rate\"].items():\n",
    "        df[f\"MC_{currency}_Billion\"] = np.round(df[\"MC_USD_Billion\"]*rate, 2)\n",
    "    df = df[table_attributes]\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_to_csv(df, output_path):\n",
    "    df.to_csv(output_path, index=False)\n",
    "\n",
    "\n",
    "def load_to_db(df, sql_connection, table_name):\n",
    "    df.to_sql(table_name, sql_connection, if_exists=\"replace\", index=False)\n",
    "\n",
    "\n",
    "def run_queries(query_statement, sql_connection):\n",
    "    log_progress(\"Running Query\")\n",
    "    print(pd.read_sql(query_statement, sql_connection))\n",
    "    log_progress(\"Querying Successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dcd830",
   "metadata": {},
   "source": [
    "## üöÄ Running the ETL Pipeline\n",
    "\n",
    "This section runs the full ETL process in sequence:\n",
    "1. **Extract** ‚Äî Pull raw HTML data from Wikipedia.  \n",
    "2. **Transform** ‚Äî Clean and enrich the dataset with currency conversions.  \n",
    "3. **Load** ‚Äî Export results to CSV and SQLite.  \n",
    "4. **Query** ‚Äî Validate and preview the loaded data.  \n",
    "\n",
    "We‚Äôll also include robust error handling and logging to track progress and issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882cae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running ETL\n",
    "\n",
    "log_progress(\"Preliminaries complete. Initiating ETL process\")\n",
    "\n",
    "try:\n",
    "    extracted = extract(url)\n",
    "    log_progress(\"Data extraction complete. Initiating Transformation process\")\n",
    "\n",
    "    transformed = transform(extracted, \"exchange_rate.csv\")\n",
    "    log_progress(\"Data transformation complete. Initiating Loading process\")\n",
    "\n",
    "    load_to_csv(transformed, output_csv_path)\n",
    "    log_progress(\"Data saved to CSV file\")\n",
    "\n",
    "    sql_conn = sqlite3.connect(database_name)\n",
    "    log_progress(\"SQL Connection initiated\")\n",
    "\n",
    "    load_to_db(transformed, sql_conn, table_name)\n",
    "    log_progress(\"Data loaded to Database as a table, Executing queries\")\n",
    "\n",
    "except Exception as e:\n",
    "    log_progress(f\"Error Found: {e}\")\n",
    "    print(f\"Error found during ETL Process: {e}\")\n",
    "\n",
    "else:\n",
    "    run_queries(\"SELECT * FROM Largest_banks\", sql_conn)\n",
    "    run_queries(\n",
    "        \"SELECT AVG(MC_GBP_Billion) FROM Largest_banks\", sql_conn)\n",
    "    run_queries(\"SELECT Name from Largest_banks LIMIT 5\", sql_conn)\n",
    "    log_progress(\"Process Complete\")\n",
    "\n",
    "finally:\n",
    "    sql_conn.close()\n",
    "    log_progress(\"Server Connection closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df590f5",
   "metadata": {},
   "source": [
    "### üîç Inspecting the Wikipedia Table\n",
    "\n",
    "Before extraction, the HTML structure of the Wikipedia page was inspected using **Developer Tools** (right-click ‚Üí *Inspect*).  \n",
    "The table we scraped is contained within a `<table>` element with the class **`wikitable`**, which we targeted using BeautifulSoup.\n",
    "\n",
    "![Wikipedia HTML Inspection](wiki_inspect.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91fbdde",
   "metadata": {},
   "source": [
    "## üß© Extraction Phase\n",
    "\n",
    "In this step, we scrape the Wikipedia page for the list of largest banks.  \n",
    "We then parse the HTML using `BeautifulSoup` and convert the table into a Pandas DataFrame for easier processing.\n",
    "\n",
    "### üßæ Sample Output:\n",
    "| Rank | Bank name                              | Total assets (2025) (US$ billion) |\n",
    "|------|----------------------------------------|-----------------------------------:|\n",
    "| 1    | Industrial and Commercial Bank of China | 6688.74 |\n",
    "| 2    | Agricultural Bank of China              | 5923.76 |\n",
    "| 3    | China Construction Bank                 | 5558.38 |\n",
    "| 4    | Bank of China                           | 4803.51 |\n",
    "| 5    | JPMorgan Chase                          | 4002.81 |\n",
    "| ...  | ...                                     | ... |\n",
    "| 96   | SEB Group                               | 339.65 |\n",
    "| 97   | Raiffeisen Group                        | 337.25 |\n",
    "| 98   | Banco Bradesco                          | 331.96 |\n",
    "| 99   | VTB Bank                                | 330.43 |\n",
    "| 100  | First Abu Dhabi Bank                    | 330.32 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0c902d",
   "metadata": {},
   "source": [
    "## üîÑ Transformation Phase\n",
    "\n",
    "In this step, we rename columns for clarity, calculate equivalent market capitalization values in GBP, EUR, and INR using exchange rates from a CSV file, and rearrange the table structure.\n",
    "\n",
    "### üßæ Sample Output (Transformed DataFrame)\n",
    "\n",
    "| Name                                   | MC_USD_Billion | MC_GBP_Billion | MC_EUR_Billion | MC_INR_Billion |\n",
    "|----------------------------------------|----------------:|----------------:|----------------:|----------------:|\n",
    "| Industrial and Commercial Bank of China | 6688.74 | 5350.99 | 6220.53 | 554830.98 |\n",
    "| Agricultural Bank of China              | 5923.76 | 4739.01 | 5509.10 | 491375.89 |\n",
    "| China Construction Bank                 | 5558.38 | 4446.70 | 5169.29 | 461067.62 |\n",
    "| Bank of China                           | 4803.51 | 3842.81 | 4467.26 | 398451.15 |\n",
    "| JPMorgan Chase                          | 4002.81 | 3202.25 | 3722.61 | 332033.09 |\n",
    "| ...                                     | ... | ... | ... | ... |\n",
    "| SEB Group                               | 339.65 | 271.72 | 315.87 | 28173.97 |\n",
    "| Raiffeisen Group                        | 337.25 | 269.80 | 313.64 | 27974.89 |\n",
    "| Banco Bradesco                          | 331.96 | 265.57 | 308.72 | 27536.08 |\n",
    "| VTB Bank                                | 330.43 | 264.34 | 307.30 | 27409.17 |\n",
    "| First Abu Dhabi Bank                    | 330.32 | 264.26 | 307.20 | 27400.04 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ea2350",
   "metadata": {},
   "source": [
    "## üíæ Loading Phase\n",
    "\n",
    "After transforming the dataset, the final step is to **load** the processed data into persistent storage formats for future analysis and querying.\n",
    "\n",
    "In this project:\n",
    "- A CSV file named **`Largest_banks_data.csv`** was created to store the cleaned and transformed data locally.  \n",
    "- The same dataset was also loaded into a **SQLite database (`Banks.db`)** under the table name **`Largest_banks`** for SQL-based validation and analytics.\n",
    "\n",
    "Both operations were logged in the `code_log.txt` file to track ETL progress and completion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95808f7",
   "metadata": {},
   "source": [
    "## üß† Query Section\n",
    "\n",
    "### üß© Query 1 ‚Äî Display All Records  \n",
    "\n",
    "**SQL Command:**\n",
    "```sql\n",
    "SELECT * FROM Largest_banks;\n",
    "```\n",
    "\n",
    "```text\n",
    "                                       Name  MC_USD_Billion  MC_GBP_Billion  MC_EUR_Billion  MC_INR_Billion\n",
    "0   Industrial and Commercial Bank of China         6688.74         5350.99         6220.53       554830.98\n",
    "1                Agricultural Bank of China         5923.76         4739.01         5509.10       491375.89\n",
    "2                   China Construction Bank         5558.38         4446.70         5169.29       461067.62\n",
    "3                             Bank of China         4803.51         3842.81         4467.26       398451.15\n",
    "4                            JPMorgan Chase         4002.81         3202.25         3722.61       332033.09\n",
    "..                                      ...             ...             ...             ...             ...\n",
    "95                                SEB Group          339.65          271.72          315.87        28173.97\n",
    "96                         Raiffeisen Group          337.25          269.80          313.64        27974.89\n",
    "97                           Banco Bradesco          331.96          265.57          308.72        27536.08\n",
    "98                                 VTB Bank          330.43          264.34          307.30        27409.17\n",
    "99                     First Abu Dhabi Bank          330.32          264.26          307.20        27400.04\n",
    "\n",
    "[100 rows x 5 columns]\n",
    "```\n",
    "\n",
    "### üßÆ Query 2 ‚Äî Average Market Cap in GBP\n",
    "\n",
    "**SQL Command:**\n",
    "```sql\n",
    "SELECT AVG(MC_GBP_Billion) FROM Largest_banks;\n",
    "```\n",
    "\n",
    "```text\n",
    "   AVG(MC_GBP_Billion)\n",
    "0              945.115\n",
    "```\n",
    "\n",
    "### üè¶ Query 3 ‚Äî Display Top 5 Banks by Name\n",
    "\n",
    "**SQL Command:**\n",
    "```sql\n",
    "SELECT Name FROM Largest_banks LIMIT 5;\n",
    "```\n",
    "\n",
    "```text\n",
    "                                      Name\n",
    "0  Industrial and Commercial Bank of China\n",
    "1               Agricultural Bank of China\n",
    "2                  China Construction Bank\n",
    "3                            Bank of China\n",
    "4                           JPMorgan Chase\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1978eaee",
   "metadata": {},
   "source": [
    "## ‚úÖ Conclusion\n",
    "\n",
    "We successfully implemented a full ETL workflow that:\n",
    "- Extracted live data from Wikipedia  \n",
    "- Transformed and enriched it with multiple currency conversions  \n",
    "- Loaded the results into both CSV and SQLite formats  \n",
    "\n",
    "This process demonstrates an **automated data pipeline** that could easily be adapted for continuous integration (CI/CD) or data engineering workflows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
