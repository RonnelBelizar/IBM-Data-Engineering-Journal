{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28d27c33",
   "metadata": {},
   "source": [
    "# üï∏Ô∏è Web Scraping with BeautifulSoup\n",
    "\n",
    "## Objective\n",
    "- Scrape a web page using Python and BeautifulSoup.\n",
    "- Extract the page title, H2 headings, and all hyperlinks.\n",
    "- Filter links starting with \"https\" and save them to a text file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d64ce5",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b343b97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4 requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e042dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b923c7f",
   "metadata": {},
   "source": [
    "## Step 2: Fetch HTML Content\n",
    "Using `requests` to get the HTML content from the Python homepage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35763424",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.python.org\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "r = requests.get(url, headers=headers)\n",
    "\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21240c8c",
   "metadata": {},
   "source": [
    "## Step 3: Extract Title, H2 Headings, and Hyperlinks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d38d7d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.python.org/psf/\n",
      "https://docs.python.org\n",
      "https://pypi.org/\n",
      "https://psfmember.org/civicrm/contribute/transact?reset=1&id=2\n",
      "https://www.linkedin.com/company/python-software-foundation/\n",
      "https://fosstodon.org/@ThePSF\n",
      "https://twitter.com/ThePSF\n",
      "https://docs.python.org/3/license.html\n",
      "https://wiki.python.org/moin/BeginnersGuide\n",
      "https://devguide.python.org/\n",
      "https://docs.python.org/faq/\n",
      "https://peps.python.org\n",
      "https://wiki.python.org/moin/PythonBooks\n",
      "https://wiki.python.org/moin/\n",
      "https://wiki.python.org/moin/PythonEventsCalendar#Submitting_an_Event\n",
      "https://docs.python.org\n",
      "https://blog.python.org\n",
      "https://pythoninsider.blogspot.com/2025/09/python-3140rc3-is-go.html\n",
      "https://pyfound.blogspot.com/2025/09/announcing-2025-psf-board-election.html\n",
      "https://pyfound.blogspot.com/2025/09/sprints-are-best-part-of-conference.html\n",
      "https://pyfound.blogspot.com/2025/09/the-2025-psf-board-election-is-open.html\n",
      "https://pyfound.blogspot.com/2025/08/pypistats-org-is-now-operated-by-the-psf.html\n",
      "https://wiki.gnome.org/Projects/PyGObject\n",
      "https://wiki.qt.io/PySide\n",
      "https://kivy.org/\n",
      "https://dearpygui.readthedocs.io/en/latest/\n",
      "https://saltproject.io\n",
      "https://www.openstack.org\n",
      "https://xon.sh\n",
      "https://docs.python.org/3/license.html\n",
      "https://wiki.python.org/moin/BeginnersGuide\n",
      "https://devguide.python.org/\n",
      "https://docs.python.org/faq/\n",
      "https://peps.python.org\n",
      "https://wiki.python.org/moin/PythonBooks\n",
      "https://wiki.python.org/moin/\n",
      "https://wiki.python.org/moin/PythonEventsCalendar#Submitting_an_Event\n",
      "https://devguide.python.org/\n",
      "https://github.com/python/cpython/issues\n",
      "https://mail.python.org/mailman/listinfo/python-dev\n",
      "https://github.com/python/pythondotorg/issues\n",
      "https://status.python.org/\n",
      "https://policies.python.org/python.org/Privacy-Notice/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "title = soup.title.string\n",
    "h2 = soup.find_all(\"h2\")\n",
    "a = soup.find_all(\"a\")\n",
    "\n",
    "hyp = []\n",
    "for link in soup.find_all(\"a\", href=True):\n",
    "    hyp.append(link[\"href\"])\n",
    "\n",
    "# print(hyp)\n",
    "\n",
    "text = []\n",
    "for link in hyp:\n",
    "    if \"https\" in link:\n",
    "        text.append(link)\n",
    "    else:\n",
    "        continue\n",
    "text_str = \"\"\n",
    "\n",
    "for line in text:\n",
    "    text_str += line + '\\n'\n",
    "\n",
    "print(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb71171",
   "metadata": {},
   "source": [
    "## Step 4: Save Filtered Links to File\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe70f1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = r\"C:\\Repos\\IBM-Data-Engineering-Journal\\exercises\\Python for Data Science, AI, & Devvelopment\\Mod_5_Web Scraping with BeautifulSoup [BeautifulSoup, open]\"\n",
    "\n",
    "with open(fr\"{directory}\\links.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(text_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e27aa4",
   "metadata": {},
   "source": [
    "## ‚úÖ Conclusion\n",
    "- Successfully extracted the title, H2 headings, and hyperlinks.\n",
    "- Filtered links starting with \"https\".\n",
    "- Saved filtered links to `links.txt` for reference.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
