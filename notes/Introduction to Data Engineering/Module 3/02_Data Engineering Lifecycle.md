# Topic: Module 3 - Data Engineering Lifecycle

---

# ğŸ“¥ Gathering and Importing Data

---

## 1. SQL (Relational Databases)
    â€¢ Pull data using SQL queries (SELECT, JOIN, WHERE).  
    â€¢ Often used for structured data in RDBMS (MySQL, PostgreSQL, Oracle).  
    â€¢ Destination â†’ Data Warehouse, Data Lake, or another SQL DB.  

---

## 2. Non-Relational Databases (NoSQL)
    â€¢ Extract data from document, key-value, columnar, or graph stores.  
    â€¢ Use APIs, drivers, or query languages (e.g., MongoDB queries, Cassandra CQL).  
    â€¢ Destination â†’ Data Lake or another NoSQL DB.  

---

## 3. APIs (Application Programming Interfaces)
    â€¢ Systems expose endpoints to request data (REST, GraphQL, SOAP).  
    â€¢ Often returns JSON or XML.  
    â€¢ Good for external data (weather, finance, healthcare APIs).  
    â€¢ Destination â†’ Databases, Warehouses, or Data Pipelines.  

---

## 4. Web Scraping
    â€¢ Extract data directly from websites (HTML parsing).  
    â€¢ Tools: BeautifulSoup, Scrapy, Selenium.  
    â€¢ Caution: Legal/ethical considerations (respect robots.txt, terms).  
    â€¢ Destination â†’ NoSQL DBs, CSV/JSON files, Data Lake.  

---

## 5. Sensor Data (IoT, Medical Devices)
    â€¢ Real-time streams from devices (IoT sensors, patient monitors, lab machines).  
    â€¢ Protocols: MQTT, OPC-UA, proprietary vendor APIs.  
    â€¢ Destination â†’ Data Lake, Time-series DB (e.g., InfluxDB), or Streaming Platform (Kafka).  

---

## 6. Data Exchange Platforms
    â€¢ Marketplaces and hubs for bulk datasets (open data portals, Kaggle, gov data).  
    â€¢ Typically downloaded in CSV, JSON, or Excel.  
    â€¢ Destination â†’ Warehouses, Data Lakes.  

---

## 7. Other Sources
    â€¢ Files â†’ CSV, XLSX, JSON, XML, Parquet.  
    â€¢ Logs â†’ Application/system logs (useful for monitoring + analytics).  
    â€¢ Emails â†’ Extract structured + unstructured info from messages.  

---

## 8. Data Types and Destinations
    â€¢ Structured â†’ SQL DBs, Warehouses.  
    â€¢ Semi-structured â†’ NoSQL DBs, Data Lakes.  
    â€¢ Unstructured â†’ Data Lakes, Object Storage (S3, GCS, Azure Blob).  
    â€¢ Real-time Streams â†’ Message Queues (Kafka, RabbitMQ), then processed into Warehouses or Lakes.  

---

## ğŸ”‘ Summary
    â€¢ Data can come from databases (SQL/NoSQL), APIs, scraping, sensors, or exchanges.  
    â€¢ Choice depends on source type, format, and business need.  
    â€¢ Destination varies: DBs for transactions, Warehouses for analytics, Data Lakes for raw/unstructured, Streams for real-time.  

---

# ğŸ§¹ Data Wrangling / Munging

---

## ğŸ“ What is Data Wrangling?
    â€¢ Process of taking raw, messy, or scattered data and turning it into a clean, structured, and usable form.  
    â€¢ Goal â†’ Make data ready for analysis, reporting, or machine learning.  
    â€¢ Why? â†’ Raw data is often incomplete, inconsistent, or in the wrong format.  
    â€¢ Think of it as: "taming wild data so it can work for you."  

---

## 1. Exploration
    â€¢ First step: understand the data before changes.  
    â€¢ Inspect data types, size, and distributions.  
    â€¢ Tools â†’ Pandas (Python), SQL queries, BI tools.  
    â€¢ Example â†’ Look at patient lab records: column types, missing values, outliers.  

---

## 2. Transformation
    â€¢ Goal: convert raw data into usable formats.  

    ğŸ”¹ Structuring  
        - Joins â†’ Combine data across tables (e.g., patient info + test results).  
        - Unions â†’ Stack datasets with same schema (e.g., monthly logs).  

    ğŸ”¹ Normalization  
        - Organize into smaller related tables to reduce redundancy.  
        - Example: Split patient demographic info from test results.  

    ğŸ”¹ Denormalization  
        - Merge tables for faster reads/analytics.  
        - Example: Combine all patient info + lab results into one wide table for reporting.  

---

## 3. Cleaning
    â€¢ Fix errors, handle missing values, and standardize.  

    ğŸ”¹ Inspection  
        - Data Profiling â†’ Stats about columns (min, max, null counts, unique values).  
        - Visualizing â†’ Graphs to spot trends/anomalies (e.g., boxplot for outliers).  

    â€¢ Example â†’ Fix misspelled hospital names, remove duplicate patient IDs.  

---

## 4. Validation
    â€¢ Ensures transformed/cleaned data meets rules + expectations.  
    â€¢ Examples:  
        - Dates must be valid (no Feb 30).  
        - IDs unique per patient.  
        - No negative ages.  
    â€¢ Tools â†’ Great Expectations, Pandera, custom scripts.  

---

## 5. Availability
    â€¢ Make processed data accessible to end-users/systems.  
    â€¢ Load into destination â†’ Warehouse, Lake, Dashboard, or API.  
    â€¢ Ensures proper permissions + uptime.  
    â€¢ Example â†’ Publish cleaned GX device data into a BI dashboard for hospitals.  

---

## ğŸ”‘ Summary
    â€¢ Wrangling = Exploration â†’ Transformation â†’ Cleaning â†’ Validation â†’ Availability.  
    â€¢ Joins/unions, normalization/denormalization shape the structure.  
    â€¢ Profiling + visualization help find issues before processing.  
    â€¢ Final data must be both clean and available for use.  

---

ğŸ’¡ Bonus:  
    â€¢ 80% of a data engineerâ€™s time often goes to wrangling/cleaning.  
    â€¢ Automation (ETL/ELT pipelines, validation frameworks) reduces manual work.  
    â€¢ Good wrangling = faster analytics + reliable insights.  

---

# ğŸ› ï¸ Tools for Data Wrangling

---

## 1. Spreadsheets / Low-Code Tools
    â€¢ Excel Power Query / Google Sheets  
        - Easy entry point for small datasets.  
        - Drag-and-drop transforms, filtering, joining.  
        - Why use â†’ Familiar, quick for ad-hoc cleaning.  
        - Why not â†’ Limited scalability, not great for big data.  

---

## 2. OpenRefine
    â€¢ Open-source tool for cleaning messy data.  
    â€¢ Great for reconciling inconsistent entries (e.g., â€œNYCâ€ vs â€œNew York Cityâ€).  
    â€¢ Why use â†’ Strong for text-heavy, dirty datasets.  
    â€¢ Why not â†’ More manual than automated pipelines.  

---

## 3. Google DataPrep (by Trifacta)
    â€¢ Cloud-based wrangling tool, tightly integrated with GCP.  
    â€¢ Smart suggestions for cleaning + transforming data.  
    â€¢ Why use â†’ Good for cloud-native teams.  
    â€¢ Why not â†’ Paid tool, tied to Google Cloud.  

---

## 4. Watson Studio Refinery
    â€¢ IBMâ€™s data prep environment, integrated into Watson Studio.  
    â€¢ Helps prep data for ML/AI workflows.  
    â€¢ Why use â†’ Good if youâ€™re in IBM ecosystem.  
    â€¢ Why not â†’ Proprietary, may be heavy for simple tasks.  

---

## 5. Trifacta Wrangler
    â€¢ Advanced wrangling platform (basis for Google DataPrep).  
    â€¢ Visual + code-based data prep for analysts/engineers.  
    â€¢ Why use â†’ Automates repetitive cleaning steps, scales well.  
    â€¢ Why not â†’ Commercial product, not free.  

---

## 6. Python (Coding Approach)
    â€¢ Jupyter Notebook â†’ Interactive coding + exploration.  
    â€¢ NumPy â†’ Fast math, array manipulation.  
    â€¢ Pandas â†’ Core library for data cleaning, joining, transforming.  
    â€¢ Why use â†’ Flexible, powerful, scalable with other libraries.  
    â€¢ Why not â†’ Requires coding knowledge.  

---

## 7. R (Coding Approach)
    â€¢ dplyr â†’ Grammar of data manipulation (filter, group, join).  
    â€¢ data.table â†’ High-performance data manipulation.  
    â€¢ jsonlite â†’ Handle JSON data easily.  
    â€¢ Why use â†’ Built for statistics + visualization.  
    â€¢ Why not â†’ Less common in data engineering pipelines than Python.  

---

## ğŸ”‘ Summary
    â€¢ Non-coding tools (Excel, OpenRefine, Trifacta) â†’ Great for beginners + smaller projects.  
    â€¢ Cloud/proprietary tools (Google DataPrep, Watson Refinery) â†’ Strong for enterprise workflows.  
    â€¢ Python + R â†’ Most flexible + scalable for engineers and data scientists.  
    â€¢ Choice depends on â†’ dataset size, complexity, and your workflow.  

---


